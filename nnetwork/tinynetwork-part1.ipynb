{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # only depency\n",
    "import numpy # import only for types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x: numpy.ndarray, deriv: bool = False) -> numpy.ndarray:\n",
    "  \"\"\"\n",
    "  This is sigmoid function. The sigmod function maps any value\n",
    "  to a value between 0 and 1.\n",
    "  In our situation we use it to convert numbers to probabilities.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  x : numpy.ndarray\n",
    "    Values for calc sigmoid. \n",
    "  deriv : bool\n",
    "    Flag for generating the derivative of a sigmoid. \n",
    "  \"\"\"\n",
    "  if deriv == True:\n",
    "    return x * (1-x)\n",
    "  return 1 / (1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Input dataset\n",
    "# This inits learning dataset as a numpy matrix. Each row is a single training example.\n",
    "# Each column corresponds to one of input nodes.\n",
    "# Thus, this model have 3 input nodes to the network and 4 training examples.\n",
    "X = np.array([\n",
    "  [0, 0, 1],\n",
    "  [0, 1, 1],\n",
    "  [1, 0, 1],\n",
    "  [1, 1, 1],\n",
    "])\n",
    "print(X)\n",
    "\n",
    "# Output dataset\n",
    "# This inits output dataset, answers. In this case dataset generated horizontally(with 1 row and 4 cols).\n",
    "# \"T\" is the transpose method. After the transpose, this matrix has 4 rows with 1 column.\n",
    "# So, this network has 3 inputs(dataset above) and 1 output.\n",
    "y = np.array([[0, 0, 1, 1]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(iter_cnt: int = 10_000) -> None:\n",
    "  \"\"\"\n",
    "  Learns the network. Here collected all training logic. Main function.\n",
    "  Parameters:\n",
    "  ----------\n",
    "  iter_cnt : int\n",
    "    Count of optimization iterations.\n",
    "  \"\"\"\n",
    "  # Seed random numbers to make calculation deterministic (it seems good practice)\n",
    "  # After this, numbers will still be randomly distributed,\n",
    "  # but they'll be randomly distributed in exactly the same way each time you train. \n",
    "  # This allows us easier see how changes affect the network.\n",
    "  np.random.seed(1)\n",
    "\n",
    "  # Initialize weights randomly with mean 0\n",
    "  # Since we only have 2 layers (input and output), we only need one matrix of weights to connect them.\n",
    "  # Its dimension is (3, 1) because we have 3 inputs and 1 output.\n",
    "  # Also inits weights as mean of zero. It's best practice  to have a mean of zero in weight initialization.\n",
    "  # Another note is that \"neural network\" is really just this matrix.\n",
    "  # All of the learning is stored in the synapse0 matrix.\n",
    "  synapse0 = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "  # This loop iterates multimple times over the training code to optimize network to the dataset.\n",
    "  for _ in range(iter_cnt):\n",
    "    # Forward propagation\n",
    "    # Since our first layer, layer0, is simply our data.\n",
    "    # This will be a \"full batch\" training, because we're going to process all of training examples in this implementation.\n",
    "    layer0 = X\n",
    "\n",
    "    # This is prediction step.\n",
    "    # This line contains 2 steps. The first matrix multiplies layer0 by synapse0.\n",
    "    # The second passes output through sigmoid function. \n",
    "    # Since we leaded in 4 training examples, we ended up with 4 guesses for the correct answer, a (4 x 1) matrix.\n",
    "    # Each output corresponds with the network's guess for a given input. \n",
    "    layer1 = nonlin(np.dot(layer0, synapse0))\n",
    "\n",
    "    # Calc error\n",
    "    # Now layer1 has a \"guess\" for each input. We can now compare how well it did\n",
    "    # by substracting the true answer (y) from the guess (layer1).\n",
    "    # So, layer1_error is just a vector of positive and negative numbers\n",
    "    # reflecting how much the network missed.\n",
    "    layer1_error = y - layer1\n",
    "\n",
    "    # Multiply how much we missed by the slope of the sigmoid at the values in layer1\n",
    "    # When we multiply the \"slopes\" by the error, we are reducing the error of high confidence predictions.\n",
    "    # If the slope was really shallow (close to 0), then the network either had a very high value, or a very low value.\n",
    "    # This means that the network was quite confident one way or the other.\n",
    "    # However, if the network guessed something close to (x=0, y=0.5) then it isn't very confident.\n",
    "    # We update these \"wishy-washy\" predictions most heavily, \n",
    "    # and we tend to leave the confident ones alone by multiplying them by a number close to 0.\n",
    "    layer1_delta = layer1_error * nonlin(layer1, True)\n",
    "\n",
    "    # Update weights\n",
    "    # It computes the weight updates for each weight for each training example,\n",
    "    # sums them, and updates the weights, all in a simple line.\n",
    "    synapse0 += np.dot(layer0.T, layer1_delta)\n",
    "\n",
    "  print(\"Training result:\")\n",
    "  print(layer1)\n",
    "  print()\n",
    "  print(\"Values that we trying to predict:\")\n",
    "  print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t__Default settings__\n",
      "Training result:\n",
      "[[0.00966449]\n",
      " [0.00786506]\n",
      " [0.99358898]\n",
      " [0.99211957]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__1 optimize iterations__\n",
      "Training result:\n",
      "[[0.2689864 ]\n",
      " [0.36375058]\n",
      " [0.23762817]\n",
      " [0.3262757 ]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__1001 optimize iterations__\n",
      "Training result:\n",
      "[[0.03176745]\n",
      " [0.02575143]\n",
      " [0.97907779]\n",
      " [0.97416005]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__2001 optimize iterations__\n",
      "Training result:\n",
      "[[0.02210122]\n",
      " [0.01793507]\n",
      " [0.985409  ]\n",
      " [0.98200541]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__3001 optimize iterations__\n",
      "Training result:\n",
      "[[0.0179128 ]\n",
      " [0.01454746]\n",
      " [0.98815758]\n",
      " [0.98540875]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__4001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01544409]\n",
      " [0.0125494 ]\n",
      " [0.98978021]\n",
      " [0.98741602]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__5001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01377152]\n",
      " [0.01119485]\n",
      " [0.99088089]\n",
      " [0.98877656]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__6001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01254323]\n",
      " [0.01019961]\n",
      " [0.99168995]\n",
      " [0.98977602]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__7001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01159234]\n",
      " [0.0094288 ]\n",
      " [0.99231678]\n",
      " [0.99054995]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__8001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01082824]\n",
      " [0.00880917]\n",
      " [0.99282079]\n",
      " [0.99117199]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__9001 optimize iterations__\n",
      "Training result:\n",
      "[[0.01019693]\n",
      " [0.00829707]\n",
      " [0.99323743]\n",
      " [0.991686  ]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "\t\t\t\t__10001 optimize iterations__\n",
      "Training result:\n",
      "[[0.009664  ]\n",
      " [0.00786466]\n",
      " [0.99358931]\n",
      " [0.99211997]]\n",
      "\n",
      "Values that we trying to predict:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "CPU times: user 386 ms, sys: 7.05 ms, total: 393 ms\n",
      "Wall time: 395 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"\\t\\t\\t\\t__Default settings__\")\n",
    "learning()\n",
    "\n",
    "for iter_cnt in range(1, 11_000, 1_000):\n",
    "  print(f\"\\t\\t\\t\\t__{iter_cnt} optimize iterations__\")\n",
    "  learning(iter_cnt=iter_cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
