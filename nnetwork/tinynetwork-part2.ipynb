{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # only depency\n",
    "import numpy # import only for types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main code of 3 Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x: numpy.ndarray, deriv: bool = False) -> numpy.ndarray:\n",
    "  \"\"\"\n",
    "  This is sigmoid function. The sigmod function maps any value\n",
    "  to a value between 0 and 1.\n",
    "  In our situation we use it to convert numbers to probabilities.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  x : numpy.ndarray\n",
    "    Values for calc sigmoid. \n",
    "  deriv : bool\n",
    "    Flag for generating the derivative of a sigmoid. \n",
    "  \"\"\"\n",
    "  if deriv == True:\n",
    "    return x * (1-x)\n",
    "  return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Input dataset\n",
    "X = np.array([\n",
    "  [0, 0, 1],\n",
    "  [0, 1, 1],\n",
    "  [1, 0, 1],\n",
    "  [1, 1, 1],\n",
    "])\n",
    "print(X)\n",
    "\n",
    "# Output dataset\n",
    "y = np.array([\n",
    "  [0],\n",
    "  [1],\n",
    "  [1],\n",
    "  [0]\n",
    "])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(iter_cnt : int = 60_000):\n",
    "  \"\"\"\n",
    "  Learns the network. Here collected all training logic. Main function.\n",
    "  Parameters:\n",
    "  ----------\n",
    "  iter_cnt : int\n",
    "    Count of optimization iterations.\n",
    "  \"\"\"\n",
    "  # Seed random numbers to make calculation deterministic (it seems good practice)\n",
    "  # After this, numbers will still be randomly distributed,\n",
    "  # but they'll be randomly distributed in exactly the same way each time you train. \n",
    "  # This allows us easier see how changes affect the network.\n",
    "  np.random.seed(1)\n",
    "\n",
    "  # Initialize weights randomly with mean 0.\n",
    "  # First layer of weights, senapse0, connecting layer0 to layer1.\n",
    "  # Its dimension is (3, 4) because we have 3 inputs and 4 outputs.\n",
    "  # Combine values into something that can have a one-to-one relationship.\n",
    "  synapse0 = 2 * np.random.random((3, 4)) - 1\n",
    "  # Second layer of wights, synapse 1 connecting layer1 to layer2.\n",
    "  # Its demnsion is (4, 1) because result of layer1 is (4, 1) matrix.\n",
    "  synapse1 = 2 * np.random.random((4, 1)) - 1\n",
    "\n",
    "  for _ in range(iter_cnt):\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    layer0 = X # Since our first layer, layer0, is simply our data.\n",
    "    layer1 = nonlin(np.dot(layer0, synapse0)) # second layer, otherwise known a hidden layer\n",
    "    layer2 = nonlin(np.dot(layer1, synapse1)) # thrid, final layer, which is our hypothesis, and should approximate the correct answer as we train.\n",
    "\n",
    "    # Calc error\n",
    "    # Now layer2 has a \"guess\" for each input. We can now compare how well it did\n",
    "    # by substracting the true answer (y) from the guess (layer2).\n",
    "    # So, layer2_error is just a vector of positive and negative numbers\n",
    "    # reflecting how much the network missed.\n",
    "    layer2_error = y - layer2\n",
    "\n",
    "    # Monitor error\n",
    "    if _ % 10_000 == 0:\n",
    "      print(\"Error:\", str(np.mean(np.abs(layer2_error))))\n",
    "\n",
    "    # This is the error of the network scaled by the confidence.\n",
    "    # It's almost identical to the error except that very confident errors are muted.\n",
    "    layer2_delta = layer2_error * nonlin(layer2, deriv=True)\n",
    "\n",
    "    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n",
    "    # Weighting layer2_delta by the weights in synapse1,\n",
    "    # we can calculate the error in the middle/hidden layer.\n",
    "    # Uses the \"confidence weighted error\" from layer2 to estabish an error for layer1.\n",
    "    # To do this, it simply sends the error across the weights from layer2 to layer1.\n",
    "    # This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2.\n",
    "    # This step is called \"backpropagating\" and is the namesake of the algorithm.\n",
    "    layer1_error = layer2_delta.dot(synapse1.T)\n",
    "\n",
    "    # This is the layer1 error of the network scaled by the confidence.\n",
    "    # Again, it's almost identical to the layer1_error except that confident errors are muted.\n",
    "    layer1_delta = layer1_error * nonlin(layer1, deriv=True)\n",
    "\n",
    "    # Update weights\n",
    "    # It computes the weight updates for each weight for each training example,\n",
    "    # sums them, and updates the weights, all in a simple line.\n",
    "    synapse1 += layer1.T.dot(layer2_delta)\n",
    "    synapse0 += layer0.T.dot(layer1_delta)\n",
    "  \n",
    "  print(\"\\nPrediction result:\")\n",
    "  print(layer2)\n",
    "  print(\"Values that we trying predict:\")\n",
    "  print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.4964100319027255\n",
      "Error: 0.008584525653247157\n",
      "Error: 0.00578945986250781\n",
      "Error: 0.004629176776769985\n",
      "Error: 0.00395876528027365\n",
      "Error: 0.0035101225678616753\n",
      "\n",
      "Prediction result:\n",
      "[[0.00260572]\n",
      " [0.99672209]\n",
      " [0.99701711]\n",
      " [0.00386759]]\n",
      "Values that we trying predict:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "CPU times: user 646 ms, sys: 9.9 ms, total: 656 ms\n",
      "Wall time: 658 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
